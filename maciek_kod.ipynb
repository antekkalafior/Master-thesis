{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5395025-279c-4c41-ab01-20018a34717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19535 images belonging to 2 classes.\n",
      "Found 4188 images belonging to 2 classes.\n",
      "Found 4188 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 11 layers, found 0 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 161\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m# Train the model\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mhistory = custom_model.fit(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    json.dump(history_dict, f)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Load pre-trained weights\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[43mcustom_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_todate.weights_aug_2000.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test data\u001b[39;00m\n\u001b[1;32m    166\u001b[0m test_loss, test_acc, test_auc, test_precision, test_recall \u001b[38;5;241m=\u001b[39m custom_model\u001b[38;5;241m.\u001b[39mevaluate(test_data_gen)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/legacy/saving/legacy_h5_format.py:357\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    355\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layer_names):\n\u001b[1;32m    364\u001b[0m     g \u001b[38;5;241m=\u001b[39m f[name]\n",
      "\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 11 layers, found 0 saved layers."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json, os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Input, Conv2D, SeparableConv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Set your batch size and number of epochs\n",
    "batch_size = 16\n",
    "nb_epochs = 20\n",
    "\n",
    "#Win 10\n",
    "train_path = 'MERGED_ALL_CLEAN_balanced_ready/train'\n",
    "val_path = 'MERGED_ALL_CLEAN_balanced_ready/val'\n",
    "test_path = 'MERGED_ALL_CLEAN_balanced_ready/test'\n",
    "\n",
    "# Initialize ImageDataGenerator without any augmentation\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the train, validation, and test data generators\n",
    "train_data_gen = datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'  ######### Changed to 'binary' for 2 classes #########\n",
    ")\n",
    "\n",
    "valid_data_gen = datagen.flow_from_directory(\n",
    "    directory=val_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'  ######### Changed to 'binary' for 2 classes #########\n",
    ")\n",
    "\n",
    "test_data_gen = datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  ######### Changed to 'binary' for 2 classes #########\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the VGG19 model without the top layers\n",
    "vgg19_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Extract weights for the desired layers\n",
    "vgg_weights = {\n",
    "    'block1_conv1': vgg19_model.get_layer('block1_conv1').get_weights(),\n",
    "    'block1_conv2': vgg19_model.get_layer('block1_conv2').get_weights(),\n",
    "    'block2_conv1': vgg19_model.get_layer('block2_conv1').get_weights(),\n",
    "    'block2_conv2': vgg19_model.get_layer('block2_conv2').get_weights()\n",
    "}\n",
    "\n",
    "def build_model():\n",
    "    input_img = Input(shape=(224,224,3), name='ImageInput')\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
    "    \n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
    "    \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.7, name='dropout1')(x)\n",
    "    x = Dense(256, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.5, name='dropout2')(x)\n",
    "    x = Dense(1, activation='sigmoid', name='fc3')(x) ######### Changed to 1 unit with 'sigmoid' activation for 2 classes #########\n",
    "    \n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "    return model\n",
    "\n",
    "custom_model =  build_model()\n",
    "#custom_model.summary()\n",
    "\n",
    "VGG_conv1_weights = vgg_weights['block1_conv1']\n",
    "VGG_conv2_weights = vgg_weights['block1_conv2']\n",
    "VGG_conv4_weights = vgg_weights['block2_conv1']\n",
    "VGG_conv5_weights = vgg_weights['block2_conv2']\n",
    "\n",
    "custom_model.get_layer('Conv1_1').set_weights(VGG_conv1_weights)\n",
    "custom_model.get_layer('Conv1_2').set_weights(VGG_conv2_weights)\n",
    "custom_model.get_layer('Conv2_1').set_weights(VGG_conv4_weights)\n",
    "custom_model.get_layer('Conv2_2').set_weights(VGG_conv5_weights)\n",
    "\n",
    "######### Freeze the transferred layers #########\n",
    "custom_model.get_layer('Conv1_1').trainable = False\n",
    "custom_model.get_layer('Conv1_2').trainable = False\n",
    "custom_model.get_layer('Conv2_1').trainable = False\n",
    "custom_model.get_layer('Conv2_2').trainable = False\n",
    "######### End of freezing #########\n",
    "\n",
    "#custom_model.summary()  \n",
    "\n",
    "# Define optimizer, early stopping, and checkpoint\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "chkpt = ModelCheckpoint(filepath='best_model_todate.weights.h5', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "######### Add additional metrics #########\n",
    "\n",
    "\n",
    "custom_model.compile(\n",
    "    loss='binary_crossentropy',  ######### Changed to 'binary_crossentropy' for 2 classes #########\n",
    "    metrics=['accuracy', AUC(name='auc'), Precision(name='precision'), Recall(name='recall')], \n",
    "    optimizer=opt\n",
    ")\n",
    "\n",
    "######### End of metrics #########\n",
    "######### End of metrics #########\n",
    "'''\n",
    "# Train the model\n",
    "history = custom_model.fit(\n",
    "    train_data_gen,\n",
    "    validation_data=valid_data_gen,\n",
    "    epochs=nb_epochs,\n",
    "    callbacks=[es, chkpt]\n",
    ")\n",
    "\n",
    "history_dict = history.history\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f)\n",
    "'''\n",
    "\n",
    "\n",
    "# Load pre-trained weights\n",
    "custom_model.load_weights('best_model_todate.weights_aug_2000.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc, test_auc, test_precision, test_recall = custom_model.evaluate(test_data_gen)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "with open('training_history_2k.json', 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "\n",
    "\n",
    "# Choose ticks every few epochs\n",
    "tick_positions = np.arange(0, len(history_dict['loss']), step=1)  # Adjust the step as needed\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='Training Loss')\n",
    "plt.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Set x-axis ticks with the chosen positions\n",
    "plt.xticks(ticks=tick_positions)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Set x-axis ticks with the chosen positions\n",
    "plt.xticks(ticks=tick_positions)\n",
    "\n",
    "plt.savefig('Training_Validation_Plot.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = custom_model.predict(test_data_gen)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_data_gen.classes, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.savefig('Confusion_Matrix.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred_prob = custom_model.predict(test_data_gen).ravel()\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(test_data_gen.classes, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_Curve.png', bbox_inches='tight')\n",
    "\n",
    "# https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=The%20area%20under%20the%20ROC,curve%20(AUC)%20of%201.0.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(test_data_gen.classes, y_pred_prob)\n",
    "average_precision = average_precision_score(test_data_gen.classes, y_pred_prob)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.step(recall, precision, where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "plt.savefig('Precision-Recall_Curve.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############### MISSCLASIFIED PICS\n",
    "\n",
    "def plot_misclassified_images(model, data_gen, num_images=5):\n",
    "    # Get predictions and true labels\n",
    "    y_pred_prob = model.predict(data_gen, verbose=1)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "    y_true = data_gen.classes\n",
    "    \n",
    "    # Generate indices\n",
    "    batch_size = data_gen.batch_size\n",
    "    steps = len(data_gen)\n",
    "    misclassified_indices = []\n",
    "\n",
    "    for i in range(steps):\n",
    "        # Get a batch of data\n",
    "        images, labels = next(data_gen)\n",
    "        predictions = model.predict(images, verbose=0).flatten()\n",
    "        batch_indices = np.where((predictions > 0.5).astype(int) != labels)[0]\n",
    "        \n",
    "        if len(batch_indices) > 0:\n",
    "            for idx in batch_indices:\n",
    "                misclassified_indices.append((i * batch_size) + idx)\n",
    "\n",
    "        if len(misclassified_indices) >= num_images:\n",
    "            break\n",
    "\n",
    "    # Plot a few misclassified images\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, idx in enumerate(misclassified_indices[:num_images]):\n",
    "        batch_idx = idx // batch_size\n",
    "        img_idx = idx % batch_size\n",
    "\n",
    "        # Get image and label\n",
    "        data_gen.reset()\n",
    "        for j in range(batch_idx + 1):\n",
    "            images, labels = next(data_gen)\n",
    "        \n",
    "        img = images[img_idx]\n",
    "        true_label = labels[img_idx]\n",
    "        pred_label = (model.predict(np.expand_dims(img, axis=0)) > 0.5).astype(int)[0][0]\n",
    "        \n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "        plt.axis('off')\n",
    "    plt.savefig('Missclassified_images.png', bbox_inches='tight')\n",
    "\n",
    "# Usage\n",
    "plot_misclassified_images(custom_model, test_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472a046c-3793-467e-8821-1e4371061097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": [\n",
      "        0.7502337098121643,\n",
      "        0.8058978319168091,\n",
      "        0.8228945136070251,\n",
      "        0.8328800797462463,\n",
      "        0.8585450649261475,\n",
      "        0.8731622099876404,\n",
      "        0.8887141942977905,\n",
      "        0.8987422585487366,\n",
      "        0.9097476005554199,\n",
      "        0.9179484844207764,\n",
      "        0.9263618588447571,\n",
      "        0.9360074996948242,\n",
      "        0.9411489963531494,\n",
      "        0.9481601119041443,\n",
      "        0.9550862312316895,\n",
      "        0.9602702260017395,\n",
      "        0.9614599943161011,\n",
      "        0.9654542207717896,\n",
      "        0.9679187536239624,\n",
      "        0.9737825989723206\n",
      "    ],\n",
      "    \"auc\": [\n",
      "        0.8005639910697937,\n",
      "        0.862351655960083,\n",
      "        0.8853951096534729,\n",
      "        0.9037367701530457,\n",
      "        0.9264711737632751,\n",
      "        0.9393771290779114,\n",
      "        0.9495150446891785,\n",
      "        0.9577470421791077,\n",
      "        0.9664487242698669,\n",
      "        0.9709383249282837,\n",
      "        0.9754785895347595,\n",
      "        0.9808979630470276,\n",
      "        0.9840002059936523,\n",
      "        0.986899197101593,\n",
      "        0.990151047706604,\n",
      "        0.9921915531158447,\n",
      "        0.9929041266441345,\n",
      "        0.9942129850387573,\n",
      "        0.9948266744613647,\n",
      "        0.9960901141166687\n",
      "    ],\n",
      "    \"loss\": [\n",
      "        0.737128496170044,\n",
      "        0.479857861995697,\n",
      "        0.42972618341445923,\n",
      "        0.39098989963531494,\n",
      "        0.3480254113674164,\n",
      "        0.31576627492904663,\n",
      "        0.2871217429637909,\n",
      "        0.2593652606010437,\n",
      "        0.23245316743850708,\n",
      "        0.2132316529750824,\n",
      "        0.19544155895709991,\n",
      "        0.16969256103038788,\n",
      "        0.15709668397903442,\n",
      "        0.13962623476982117,\n",
      "        0.11846324801445007,\n",
      "        0.10679809004068375,\n",
      "        0.09947683662176132,\n",
      "        0.09044067561626434,\n",
      "        0.0837690457701683,\n",
      "        0.07221648097038269\n",
      "    ],\n",
      "    \"precision\": [\n",
      "        0.7872402667999268,\n",
      "        0.9184005856513977,\n",
      "        0.9350738525390625,\n",
      "        0.9449114203453064,\n",
      "        0.9488297700881958,\n",
      "        0.948977530002594,\n",
      "        0.9517084956169128,\n",
      "        0.9515011310577393,\n",
      "        0.9505653381347656,\n",
      "        0.9503663182258606,\n",
      "        0.9492299556732178,\n",
      "        0.9533445239067078,\n",
      "        0.9558306932449341,\n",
      "        0.9627094864845276,\n",
      "        0.9700667262077332,\n",
      "        0.9737578630447388,\n",
      "        0.9737393260002136,\n",
      "        0.9776731133460999,\n",
      "        0.9789491891860962,\n",
      "        0.9828512072563171\n",
      "    ],\n",
      "    \"recall\": [\n",
      "        0.6858162879943848,\n",
      "        0.6714540719985962,\n",
      "        0.6939746737480164,\n",
      "        0.706977128982544,\n",
      "        0.7579671740531921,\n",
      "        0.7887312173843384,\n",
      "        0.8189852833747864,\n",
      "        0.840316116809845,\n",
      "        0.8644514083862305,\n",
      "        0.8819580078125,\n",
      "        0.9009093046188354,\n",
      "        0.9168862104415894,\n",
      "        0.925044596195221,\n",
      "        0.9324381947517395,\n",
      "        0.9391518831253052,\n",
      "        0.9460355043411255,\n",
      "        0.9485000371932983,\n",
      "        0.9526642560958862,\n",
      "        0.9564034938812256,\n",
      "        0.9643919467926025\n",
      "    ],\n",
      "    \"val_accuracy\": [\n",
      "        0.865807056427002,\n",
      "        0.8784622550010681,\n",
      "        0.8765520453453064,\n",
      "        0.8784622550010681,\n",
      "        0.9133237600326538,\n",
      "        0.9195320010185242,\n",
      "        0.9190544486045837,\n",
      "        0.9154728055000305,\n",
      "        0.9243075251579285,\n",
      "        0.9352912902832031,\n",
      "        0.9338586330413818,\n",
      "        0.9407832026481628,\n",
      "        0.9386342167854309,\n",
      "        0.9400668740272522,\n",
      "        0.9443648457527161,\n",
      "        0.950095534324646,\n",
      "        0.9386342167854309,\n",
      "        0.9486628174781799,\n",
      "        0.9372014999389648,\n",
      "        0.9472301602363586\n",
      "    ],\n",
      "    \"val_auc\": [\n",
      "        0.9306038618087769,\n",
      "        0.9514241218566895,\n",
      "        0.9621927738189697,\n",
      "        0.9673420786857605,\n",
      "        0.9708999395370483,\n",
      "        0.9755796790122986,\n",
      "        0.9799085259437561,\n",
      "        0.9801079630851746,\n",
      "        0.9831498265266418,\n",
      "        0.9836217761039734,\n",
      "        0.9863463640213013,\n",
      "        0.9871200323104858,\n",
      "        0.9862828850746155,\n",
      "        0.9876863956451416,\n",
      "        0.9874945282936096,\n",
      "        0.9879772663116455,\n",
      "        0.9875530004501343,\n",
      "        0.9871156811714172,\n",
      "        0.9841448664665222,\n",
      "        0.9848490357398987\n",
      "    ],\n",
      "    \"val_loss\": [\n",
      "        0.39719364047050476,\n",
      "        0.285289466381073,\n",
      "        0.2629457712173462,\n",
      "        0.26069149374961853,\n",
      "        0.23394463956356049,\n",
      "        0.20131275057792664,\n",
      "        0.18743276596069336,\n",
      "        0.19268538057804108,\n",
      "        0.18052229285240173,\n",
      "        0.16124464571475983,\n",
      "        0.16483241319656372,\n",
      "        0.14719678461551666,\n",
      "        0.1507527083158493,\n",
      "        0.14850638806819916,\n",
      "        0.15469805896282196,\n",
      "        0.1355355829000473,\n",
      "        0.16285830736160278,\n",
      "        0.1485975831747055,\n",
      "        0.20279152691364288,\n",
      "        0.17889662086963654\n",
      "    ],\n",
      "    \"val_precision\": [\n",
      "        0.8952528238296509,\n",
      "        0.9546758532524109,\n",
      "        0.9718731045722961,\n",
      "        0.9703264236450195,\n",
      "        0.9269856810569763,\n",
      "        0.9558899998664856,\n",
      "        0.9715206623077393,\n",
      "        0.9733405709266663,\n",
      "        0.978459894657135,\n",
      "        0.9492360949516296,\n",
      "        0.9829877614974976,\n",
      "        0.9723643660545349,\n",
      "        0.9688616394996643,\n",
      "        0.9782044887542725,\n",
      "        0.9740193486213684,\n",
      "        0.9537795186042786,\n",
      "        0.9846965670585632,\n",
      "        0.9761784076690674,\n",
      "        0.9867091774940491,\n",
      "        0.9756221175193787\n",
      "    ],\n",
      "    \"val_recall\": [\n",
      "        0.8285577893257141,\n",
      "        0.7946513891220093,\n",
      "        0.7755491733551025,\n",
      "        0.780802309513092,\n",
      "        0.8973256945610046,\n",
      "        0.8796561360359192,\n",
      "        0.8634192943572998,\n",
      "        0.8543457388877869,\n",
      "        0.8677172660827637,\n",
      "        0.9197707772254944,\n",
      "        0.882999062538147,\n",
      "        0.9073543548583984,\n",
      "        0.9063992500305176,\n",
      "        0.9001910090446472,\n",
      "        0.9130849838256836,\n",
      "        0.9460362792015076,\n",
      "        0.8911174535751343,\n",
      "        0.9197707772254944,\n",
      "        0.88634192943573,\n",
      "        0.9173830151557922\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_history_2k.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the content of the JSON file\n",
    "print(json.dumps(data, indent=4))  # Pretty print with indentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
